{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36041a64",
   "metadata": {},
   "source": [
    "# Install all the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33716128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing required packages with correct Pinecone setup...\")\n",
    "\n",
    "%pip install -q wikipedia-api\n",
    "%pip install -q langchain\n",
    "%pip install -q langchain-openai\n",
    "%pip install -q langchain-community\n",
    "%pip install -q langchain-text-splitters\n",
    "%pip install -q langchainhub\n",
    "%pip install -q pinecone \n",
    "%pip install -q langchain-pinecone\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q tiktoken\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72b73b",
   "metadata": {},
   "source": [
    "# Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cfe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing libraries...\")\n",
    "\n",
    "import re\n",
    "import random\n",
    "from getpass import getpass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Wikipedia\n",
    "import wikipediaapi\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Pinecone - CORRECT import\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca68f91",
   "metadata": {},
   "source": [
    "# Set enviornment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up environment variables...\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Set environment variables securely\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"<PINECONE_API_KEY>\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<AZURE_OPENAI_API_KEY>\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<AZURE_OPENAI_ENDPOINT>\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "\n",
    "print(\"âœ… Environment variables set successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2d349",
   "metadata": {},
   "source": [
    "# Task 1: Fetch and Parse Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc65e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles fetching and cleaning Wikipedia content\n",
    "\n",
    "class WikipediaProcessor:    \n",
    "    def __init__(self):\n",
    "        self.wiki_wiki = wikipediaapi.Wikipedia(\n",
    "            user_agent='RAG-Pipeline/1.0',\n",
    "            language='en',\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        )\n",
    "    \n",
    "    # Clean Wikipedia text by removing references and formatting artifacts.\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove Wikipedia references (e.g., [1], [citation needed])\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        text = re.sub(r'\\[citation needed\\]', '', text)\n",
    "        text = re.sub(r'\\[who\\]', '', text)\n",
    "        text = re.sub(r'\\[when\\]', '', text)\n",
    "        \n",
    "        # Remove text within parentheses and square brackets\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "        \n",
    "        # Remove section headers\n",
    "        text = re.sub(r'=+\\s*(.*?)\\s*=+', r'\\1', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # Fetch multiple Wikipedia pages and return cleaned content.\n",
    "    def fetch_pages(self, page_titles: List[str]) -> Dict:\n",
    "        results = {}\n",
    "        \n",
    "        for title in page_titles:\n",
    "            try:\n",
    "                print(f\"Fetching: {title}\")\n",
    "                page = self.wiki_wiki.page(title)\n",
    "                if page.exists():\n",
    "                    cleaned_content = self.clean_text(page.text)\n",
    "                    results[title] = {\n",
    "                        'content': cleaned_content,\n",
    "                        'url': page.fullurl,\n",
    "                        'title': page.title,\n",
    "                        'length': len(cleaned_content)\n",
    "                    }\n",
    "                    print(f\"Successfully fetched: {title} ({len(cleaned_content)} chars)\")\n",
    "                else:\n",
    "                    print(f\"Page not found: {title}\")\n",
    "                    results[title] = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {title}: {e}\")\n",
    "                results[title] = None\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize processor and fetch pages\n",
    "wiki_processor = WikipediaProcessor()\n",
    "page_titles = ['Artificial_intelligence', 'Machine_learning']\n",
    "\n",
    "\n",
    "print(\"Starting Wikipedia content fetch...\")\n",
    "wikipedia_data = wiki_processor.fetch_pages(page_titles)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WIKIPEDIA CONTENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for title, data in wikipedia_data.items():\n",
    "    if data:\n",
    "        print(f\" {data['title']}:\")\n",
    "        print(f\"  URL: {data['url']}\")\n",
    "        print(f\"  Length: {data['length']:,} characters\")\n",
    "        print(f\"  Preview: {data['content'][:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1607312",
   "metadata": {},
   "source": [
    "# Task 2: Chunking with Randomized Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55059d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Custom text splitter with randomized chunk sizes.\n",
    "class RandomizedTextSplitter:\n",
    "    \n",
    "    def __init__(self, chunk_size_range: tuple = (400, 600), chunk_overlap: int = 50, length_function=len):\n",
    "        self.chunk_size_range = chunk_size_range\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.length_function = length_function\n",
    "    \n",
    "    # Split text into chunks with randomized sizes.\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        if self.length_function(text) <= self.chunk_size_range[0]:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < self.length_function(text):\n",
    "            # Randomly select chunk size within range\n",
    "            chunk_size = random.randint(*self.chunk_size_range)\n",
    "            end = min(start + chunk_size, len(text))\n",
    "            \n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            start = end - self.chunk_overlap\n",
    "            \n",
    "            if start >= len(text) - self.chunk_overlap:\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Function to create document chunks from Wikipedia data\n",
    "def create_chunks(wikipedia_data: Dict, use_randomized: bool = True) -> List[Document]:\n",
    "    all_chunks = []\n",
    "    \n",
    "    if use_randomized:\n",
    "        # Use custom randomized splitter\n",
    "        text_splitter = RandomizedTextSplitter(\n",
    "            chunk_size_range=(400, 600),\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "    else:\n",
    "        # Use standard recursive splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    for title, data in wikipedia_data.items():\n",
    "        if data and data['content']:\n",
    "            if use_randomized:\n",
    "                chunks = text_splitter.split_text(data['content'])\n",
    "            else:\n",
    "                chunks = text_splitter.split_text(data['content'])\n",
    "            \n",
    "            # Create Document objects with metadata\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": data['url'],\n",
    "                        \"title\": data['title'],\n",
    "                        \"chunk_id\": i,\n",
    "                        \"source_type\": \"wikipedia\",\n",
    "                        \"chunk_size\": len(chunk)\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(doc)\n",
    "            \n",
    "            print(f\"Created {len(chunks)} chunks from {title}\")\n",
    "            if use_randomized:\n",
    "                chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "                print(f\"   Chunk size range: {min(chunk_sizes)}-{max(chunk_sizes)} characters\")\n",
    "    \n",
    "    print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
    "    return all_chunks\n",
    "\n",
    "# Create chunks with randomized sizes\n",
    "print(\"Creating chunks with randomized sizes (400-600 characters)...\")\n",
    "all_chunks = create_chunks(wikipedia_data, use_randomized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e96f21",
   "metadata": {},
   "source": [
    "# Task 3: Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manages Pinecone vector store operations\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pc = None\n",
    "        self.embeddings = None\n",
    "        self.vector_store = None\n",
    "    \n",
    "    # Initialize Azure OpenAI embeddings.\n",
    "    def initialize_embeddings(self):\n",
    "        try:\n",
    "            self.embeddings = AzureOpenAIEmbeddings(\n",
    "                azure_deployment=\"text-embedding-3-small\",\n",
    "                openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                chunk_size=1000\n",
    "            )\n",
    "            print(\"Azure OpenAI Embeddings initialized successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing embeddings: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Initialize Pinecone and create index if needed\n",
    "    def initialize_pinecone(self, index_name: str = \"rag-pipeline\"):\n",
    "        try:\n",
    "            self.pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "            print(\"Pinecone client initialized successfully\")\n",
    "            \n",
    "            # Create index if it doesn't exist\n",
    "            if index_name not in self.pc.list_indexes().names():\n",
    "                print(f\"Creating new index: {index_name}\")\n",
    "                self.pc.create_index(\n",
    "                    name=index_name,\n",
    "                    dimension=1536,  # Dimension for text-embedding-ada-002\n",
    "                    metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud=\"aws\",\n",
    "                        region=\"us-east-1\"\n",
    "                    )\n",
    "                )\n",
    "                # Wait for index to be ready\n",
    "                import time\n",
    "                time.sleep(10)\n",
    "\n",
    "                print(f\"Created new index: {index_name}\")\n",
    "            else:\n",
    "                print(f\"Using existing index: {index_name}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Pinecone: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Store documents in Pinecone vector store\n",
    "    def store_documents(self, documents: List[Document], index_name: str = \"rag-pipeline\"):\n",
    "        try:\n",
    "            if not documents:\n",
    "                print(\"No documents to store\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"Storing {len(documents)} documents in Pinecone...\")\n",
    "            \n",
    "            self.vector_store = PineconeVectorStore.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embeddings,\n",
    "                index_name=index_name\n",
    "            )\n",
    "            \n",
    "            print(\"Documents successfully stored in Pinecone!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing documents: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize vector store manager\n",
    "vector_manager = VectorStoreManager()\n",
    "\n",
    "# Initialize embeddings\n",
    "print(\"Initializing Azure OpenAI Embeddings...\")\n",
    "vector_manager.initialize_embeddings()\n",
    "\n",
    "# Initialize Pinecone\n",
    "print(\"\\nInitializing Pinecone...\")\n",
    "vector_manager.initialize_pinecone()\n",
    "\n",
    "# Store documents in Pinecone\n",
    "print(\"\\nStoring documents in Pinecone...\")\n",
    "vector_manager.store_documents(all_chunks)\n",
    "\n",
    "# Create retriever for later use\n",
    "retriever = vector_manager.vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c805fe",
   "metadata": {},
   "source": [
    "# Task 4: Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search the vector database with optional source filtering\n",
    "\n",
    "def search_db(query: str, k: int = 3, filter_source: Optional[str] = None) -> List[Dict]:\n",
    "    try:\n",
    "        # Perform similarity search\n",
    "        results = vector_manager.vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        # Apply source filter if provided\n",
    "        if filter_source:\n",
    "            results = [doc for doc in results if doc.metadata.get(\"source\") == filter_source]\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for i, doc in enumerate(results):\n",
    "            formatted_results.append({\n",
    "                \"rank\": i + 1,\n",
    "                \"content\": doc.page_content,\n",
    "                \"source_url\": doc.metadata.get(\"source\"),\n",
    "                \"title\": doc.metadata.get(\"title\"),\n",
    "                \"chunk_size\": doc.metadata.get(\"chunk_size\", \"N/A\")\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the search function\n",
    "print(\"Testing similarity search...\")\n",
    "\n",
    "# Get source URLs for filtering\n",
    "ai_url = wikipedia_data['Artificial_intelligence']['url']\n",
    "ml_url = wikipedia_data['Machine_learning']['url']\n",
    "\n",
    "test_queries = [\n",
    "    \"Who is considered the father of AI?\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"What are neural networks?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Unrestricted search\n",
    "    results = search_db(query, k=2)\n",
    "    print(f\"Unrestricted results ({len(results)}):\")\n",
    "    for result in results:\n",
    "        print(f\"  ðŸ“ {result['rank']}. {result['content'][:100]}...\")\n",
    "        print(f\"     ðŸ”— Source: {result['source_url']}\")\n",
    "    \n",
    "    # AI-only search\n",
    "    ai_results = search_db(query, k=2, filter_source=ai_url)\n",
    "    print(f\"\\nAI-only results ({len(ai_results)}):\")\n",
    "    for result in ai_results:\n",
    "        print(f\"  ðŸ“ {result['rank']}. {result['content'][:100]}...\")\n",
    "    \n",
    "    # ML-only search\n",
    "    ml_results = search_db(query, k=2, filter_source=ml_url)\n",
    "    print(f\"\\nML-only results ({len(ml_results)}):\")\n",
    "    for result in ml_results:\n",
    "        print(f\"  ðŸ“ {result['rank']}. {result['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8893a24",
   "metadata": {},
   "source": [
    "# Task 5: LLM Module with RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357434b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG QA System with source citations\n",
    "\n",
    "class RAGQASystem:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.qa_chain = None\n",
    "        self.initialize_llm()\n",
    "    \n",
    "    # Initialize Azure Chat OpenAI LLM\n",
    "    def initialize_llm(self):\n",
    "        try:\n",
    "            self.llm = AzureChatOpenAI(\n",
    "                azure_deployment=\"gpt-35-turbo\",  # Replace with your deployment name\n",
    "                api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                temperature=0.0,  # Low temperature for factual responses\n",
    "                max_tokens=500\n",
    "            )\n",
    "            print(\"Azure Chat OpenAI initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing LLM: {e}\")\n",
    "    \n",
    "    # Create RetrievalQA chain with source citations\n",
    "    def create_qa_chain(self):\n",
    "\n",
    "        prompt_template = \"\"\"You are a helpful AI assistant. Use ONLY the following retrieved context to answer the question. \n",
    "        If the answer is not in the context, say \"I cannot answer this question based on the provided information.\"\n",
    "        If the answer comes from multiple sources, cite all relevant URLs.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                #retriever=vector_manager.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                retriever=retriever,\n",
    "                chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(\"RetrievalQA chain created successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating QA chain: {e}\")\n",
    "    \n",
    "    # Ask a question and get answer with sources\n",
    "    def ask_question(self, query: str) -> Dict:\n",
    "        if not self.qa_chain:\n",
    "            self.create_qa_chain()\n",
    "        \n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Extract unique sources\n",
    "            sources = list(set(\n",
    "                doc.metadata.get(\"source\", \"Unknown\") \n",
    "                for doc in result[\"source_documents\"]\n",
    "            ))\n",
    "            \n",
    "            return {\n",
    "                \"question\": query,\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": sources,\n",
    "                \"source_documents\": result[\"source_documents\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": query,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "# Initialize QA system\n",
    "print(\"Initializing RAG QA System...\")\n",
    "qa_system = RAGQASystem()\n",
    "qa_system.create_qa_chain()\n",
    "\n",
    "# Test the QA system\n",
    "print(\"\\n Testing QA System...\")\n",
    "test_questions = [\n",
    "    \"Who is considered the father of AI?\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"What are the main differences between AI and machine learning?\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = qa_system.ask_question(question)\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Sources:\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"   - {source}\")\n",
    "    \n",
    "    print(f\"\\n Retrieved chunks used:\")\n",
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        print(f\"   {i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260ef26",
   "metadata": {},
   "source": [
    "# Task 6: Extended System with Page-Specific Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended RAG System with Page-Specific Search\n",
    "class ExtendedRAGSystem(RAGQASystem):\n",
    "    \n",
    "    def __init__(self, wikipedia_data: Dict):\n",
    "        super().__init__()\n",
    "        self.wikipedia_data = wikipedia_data\n",
    "        self.ai_url = wikipedia_data['Artificial_intelligence']['url']\n",
    "        self.ml_url = wikipedia_data['Machine_learning']['url']\n",
    "    \n",
    "    # Answer question only from AI page\n",
    "    def ask_ai(self, query: str) -> Dict:\n",
    "        return self._ask_page_specific(query, self.ai_url, \"AI\")\n",
    "    \n",
    "    # Answer question only from ML page\n",
    "    def ask_ml(self, query: str) -> Dict:\n",
    "        return self._ask_page_specific(query, self.ml_url, \"ML\")\n",
    "    \n",
    "    # Core method to handle page-specific queries\n",
    "    def _ask_page_specific(self, query: str, source_url: str, page_name: str) -> Dict:\n",
    "        try:\n",
    "            # Get relevant chunks from specific source\n",
    "            results = search_db(query, k=3, filter_source=source_url)\n",
    "            \n",
    "            if not results:\n",
    "                return {\n",
    "                    \"question\": query,\n",
    "                    \"answer\": f\"I cannot answer this question based on the {page_name} page information alone.\",\n",
    "                    \"sources\": [],\n",
    "                    \"source_documents\": []\n",
    "                }\n",
    "            \n",
    "            # Combine context from filtered results\n",
    "            context = \"\\n\\n\".join([f\"Source: {r['source_url']}\\nContent: {r['content']}\" for r in results])\n",
    "            \n",
    "            # Create prompt for page-specific answers\n",
    "            prompt = f\"\"\"You are a helpful AI assistant. Use ONLY the following context from the {page_name} Wikipedia page to answer the question. \n",
    "            If the answer is not in this context, say \"I cannot answer this question based on the {page_name} page information alone.\"\n",
    "\n",
    "            Context from {page_name} page:\n",
    "            {context}\n",
    "\n",
    "            Question: {query}\n",
    "\n",
    "            Answer:\"\"\"\n",
    "            \n",
    "            # Get LLM response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            answer = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            return {\n",
    "                \"question\": query,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [source_url],\n",
    "                \"source_documents\": results\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": query,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "    \n",
    "    def compare_retrieval_methods(self, query: str):\n",
    "        \"\"\"Compare answers from different retrieval methods\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COMPARISON FOR: '{query}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Unrestricted retrieval\n",
    "        print(f\"\\n1. UNRESTRICTED RETRIEVAL (all pages):\")\n",
    "        unrestricted_result = self.ask_question(query)\n",
    "        print(f\"   Answer: {unrestricted_result['answer']}\")\n",
    "        print(f\"   Sources: {len(unrestricted_result['sources'])} sources\")\n",
    "        \n",
    "        # AI-only retrieval\n",
    "        print(f\"\\n2. AI-ONLY RETRIEVAL:\")\n",
    "        ai_result = self.ask_ai(query)\n",
    "        print(f\"   Answer: {ai_result['answer']}\")\n",
    "        print(f\"   Sources: {len(ai_result['sources'])} source(s)\")\n",
    "        \n",
    "        # ML-only retrieval\n",
    "        print(f\"\\n3. ML-ONLY RETRIEVAL:\")\n",
    "        ml_result = self.ask_ml(query)\n",
    "        print(f\"   Answer: {ml_result['answer']}\")\n",
    "        print(f\"   Sources: {len(ml_result['sources'])} source(s)\")\n",
    "\n",
    "# Initialize extended system\n",
    "print(\"Initializing Extended RAG System...\")\n",
    "extended_system = ExtendedRAGSystem(wikipedia_data)\n",
    "\n",
    "# Test the extended system\n",
    "print(\"\\n Testing Page-Specific Search...\")\n",
    "comparison_queries = [\n",
    "    \"What is the definition and history?\",\n",
    "    \"What are the main types or categories?\",\n",
    "    \"What are the applications and future directions?\"\n",
    "]\n",
    "\n",
    "for query in comparison_queries:\n",
    "    extended_system.compare_retrieval_methods(query)\n",
    "\n",
    "# Specific test cases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFIC TEST CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "specific_tests = [\n",
    "    (\"What is the Turing Test?\", \"Tests AI page knowledge\"),\n",
    "    (\"What is reinforcement learning?\", \"Tests ML page knowledge\"),\n",
    "    (\"What are the ethical concerns?\", \"Tests cross-page knowledge\")\n",
    "]\n",
    "\n",
    "for query, description in specific_tests:\n",
    "    print(f\"\\n {description}: '{query}'\")\n",
    "    extended_system.compare_retrieval_methods(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
